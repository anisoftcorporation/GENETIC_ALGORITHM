{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00307ce",
   "metadata": {},
   "source": [
    "## Anomaly Detection with Negative Selection\n",
    "\n",
    "### Project description:\n",
    "\n",
    "In this project, you will use the same Textor algorithm from the previous project to detect anomalies in a different data set. We will use a fetal monitoring dataset (known as a cardiotocography) from the UC Irvine Machine Learning Repository. \n",
    "\n",
    "### Project goals:\n",
    "\n",
    "1. Apply the Textor algorithm to a practical example\n",
    "2. Use ROC analysis to score the detector\n",
    "\n",
    "### Project question overview:\n",
    "\n",
    "1. Write a function to process the cardiotocography data set. [Question 1](#question1)\n",
    "2. Paste any previous functions or write additional helper functions (not-graded). [Question 2](#question2)\n",
    "3. Write a function to calculate AUC values over an interval of $r$ values. [Question 3](#question3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5a0c9",
   "metadata": {},
   "source": [
    "## Cardiotocography Dataset\n",
    "\n",
    "A cardiotocography is a technical way to measure the fetal heart rate (FHR) and the uterine contractions (UC) during pregnancy. Obstetricians then classify these readings as either normal, suspect, and pathologic. Figure 1 shows the display of a cardiotocograph (CTG), the FHR is shown in orange and the UC is shown in green. Figure 2 shows the output of a typical CTG where the line labeled **A** is the FHR and the line labeled **D** is the UC. More details on the data set are here: http://odds.cs.stonybrook.edu/cardiotocogrpahy-dataset/. \n",
    "\n",
    "The data consists of 21 real-valued variables, outliers, and inliers. Each row is a sample, each column is an observed variable, and the training file includes the ground truth labels of the samples. The data are in: ``cardio_train.csv`` and ``cardio_test.csv``. \n",
    "\n",
    "<table><tr>\n",
    "<td> \n",
    "  <p align=\"center\" style=\"padding: 10px\">\n",
    "    <img alt=\"Forwarding\" src=\"images/cardio.jpg\" width=\"250\">\n",
    "    <br>\n",
    "    <em style=\"color: grey\">Figure 1: Cardiotocograph display</em>\n",
    "  </p> \n",
    "</td>\n",
    "<td> \n",
    "  <p align=\"center\">\n",
    "    <img alt=\"Routing\" src=\"images/CTG_Output.jpg\" width=\"650\">\n",
    "    <br>\n",
    "    <em style=\"color: grey\">Figure 2: Cardiotocograph output</em>\n",
    "  </p> \n",
    "</td>\n",
    "</tr></table>\n",
    "\n",
    "*Image sources: Wikipedia*\n",
    "\n",
    "\n",
    "\n",
    "## Data Processing\n",
    "\n",
    "In order for the continuous data to work well with the negative selection algorithm, we will need to bin the values into categorical variables. For consistency across assignments, we will simply bin each of the variables into 10 bins. This will produce strings of length 21 symbols and an alphabet size of 10 characters (e.g. A indicates the lowest category and J indicates the highest). \n",
    "\n",
    "Next, we will need to format the data in such a way that the new train and test data may be read directly by Textor's algorithm used in the previous project. Recall that both the train and the test data for the languages were ``.csv`` files with one string of length 10 per line. Here, we would like there to be one string of length 21 per line. These strings will correspond to the 21 variables in a row of either ``cardio_train.csv`` or ``cardio_test.csv``. \n",
    "\n",
    "### Creating Bins with pandas\n",
    "\n",
    "To avoid ambiguity in how the data can be binned, we suggest that you use the ``cut`` function from pandas. Note that this function allows for a label parameter to be passed. To map our binned data to characters we can define the labels array as follows:\n",
    "\n",
    "``labels=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']``.\n",
    "\n",
    "After defining labels we can call ``pandas.cut(x, bins=10, labels)``, where ``x`` is the data column.\n",
    "\n",
    "<a id='question1'></a>\n",
    "# Question 1\n",
    "\n",
    "Write the function ``process_data(file)`` which processes either ``cardio_train.csv`` or ``cardio_test.csv`` into the format specified in the \"Data Processing\" section. This function returns the string ``processed_file`` which is a path to the final processed file. This function will not be directly graded but will be used in the grading of later questions. The processed file should be in ``.csv`` format with one string of length 21 per row. The final processed file should look similar to the strings below. \n",
    "\n",
    "**Do not include headers or indices in your final processed file. The file should contain only strings.**\n",
    "\n",
    "```\n",
    "EEDAECEAACACIBJDBEDDA\n",
    "EEBAFCEAACACIBJCBEDDA\n",
    "EEBAFBEAADAEHAFGAEDDA\n",
    "EEDAFAEAADADHAFFAEDDA\n",
    "EECBFBEACBACDDDCAEDDA\n",
    "EEEEEAEACBABFBEBAEDEA\n",
    "DDDICDECBCABGBGDAFCDE\n",
    "DDCHCCEEBCABGBGBAECDC\n",
    "DDDJDCEABCACIAHCAFCDD\n",
    "DDBJDCEFBCABGAEDADCCC\n",
    "DDCJDCEDCBABHBGDBDCDB\n",
    "DDCICCEFCBABJAJFADCCB\n",
    "EEACAHEACIAAJAJFADAAF\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da08600a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done_cardio_test.csv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_data(file):\n",
    "    '''\n",
    "    Function to process the data in a given file. \n",
    "    Inputs:\n",
    "        file: path to either test or train file\n",
    "    Outputs:\n",
    "        processed_file: path to processed version of the file\n",
    "    '''\n",
    "    processed_file = 'done_'+file\n",
    "    labels=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "    count=0\n",
    "    df = pd.read_csv(file) \n",
    "    \n",
    "    sliced_df = df.loc[:, 'x_0':'x_20']\n",
    "    for col in sliced_df.columns:\n",
    "    # Ensure the column is numerical before applying cut\n",
    "        if pd.api.types.is_numeric_dtype(sliced_df[col]):\n",
    "            sliced_df[col + '_binned'] = pd.cut(sliced_df[col], bins=10, labels=labels)\n",
    "    \n",
    "    \n",
    "    sliced_df['combined_string'] = sliced_df.loc[:, 'x_0_binned':'x_20_binned'].astype(str).agg(''.join, axis=1)\n",
    "    \n",
    "    sliced_df['combined_string'].to_csv(processed_file,header=False, index=False)\n",
    "    return processed_file\n",
    "process_data(\"cardio_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb84402",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Implementing Negative Selection Recap\n",
    "\n",
    "Recall from the previous project that we used a Python subprocess to use Textor's ``negsel.jar`` file. To start a subprocess, we will first need to import the ``subprocess`` module and the ``run`` and ``PIPE`` submodules. Then, we will need to run the process with an array of arguments and the opened input file. We will use the Textor example ([here](http://johannes-textor.name/negativeselection.html)) to demonstrate how subprocesses work.\n",
    "\n",
    "The following command trains the negative selection algorithm on the ``english.train`` data set and tests on the ``hiligaynon.test`` data set from the previous project.\n",
    "\n",
    "``java -jar negsel.jar -c -n 10 -r 4 -self english.train < hiligaynon.test ``\n",
    "\n",
    "In this project, you will use the processed train and test data to run this same command. Recall that to run this subprocess, we will need to first build an argument list from the command. This allows the subprocess command to parse the command and flags. Using the command given above, we will construct the following argument list. \n",
    "\n",
    "``args = ['java', '-jar', 'negsel.jar', '-c', '-n', '10', '-r','4', '-self', 'negsel_src/tests/english.train']``\n",
    "\n",
    "Next, we will need to open the test file to read from. This step is necessary as the argument list will not recognize the redirection from stdin (``< hiligaynon.test``).\n",
    "\n",
    "``input_file = open('negsel_src/tests/hiligaynon.test', 'r')``\n",
    "\n",
    "Finally, we can use the ``run`` submodule to call the command and pipe the output. \n",
    "\n",
    "``result = run(args, stdout=PIPE, stdin=input_file, universal_newlines=True)``\n",
    "\n",
    "The final results will be stored in ``result.stdout``. This can be saved and parsed later as needed. \n",
    "\n",
    "Refer to the previous project for more information on how to run the code as well as a full running example.\n",
    "\n",
    "**REMEMBER TO ADJUST THE STRING LENGTH FOR THIS PROJECT.**\n",
    "\n",
    "## Exploring Model Parameterization\n",
    "\n",
    "In the previous project, we used $r=4$ contiguous bits for all of our testing. However, this does not ensure optimal anomaly detection. In this project, we will investigate the tuning of the $r$ parameter for the *$r$-contiguous* patterns. \n",
    " \n",
    "\n",
    "### $r$-contiguous Patterns\n",
    "\n",
    "Textor in _\"A Comparative Study of Negative Selection Based Anomaly Detection in Sequence Data\"_ defines the $r$-contiguous pattern as follows.\n",
    "\n",
    "<blockquote>An $r$-contiguous pattern is a string $d\\in \\Sigma^l$. It matches another string $s\\in  \\Sigma^l$ if $d$ and $s$ are identical in at least $r$ contiguous positions, i.e., if there is an $i\\in \\{1,...,l-r+1\\}$ such that the substrings of length $r$ of $s$ and $d$ starting at the $i$-th position are equal. </blockquote> \n",
    "\n",
    "### Scoring the Detector \n",
    "\n",
    "Since our strings are of length 21, we will consider $r\\in [2,10]$. For simplicity, we will use the *AUC* as described in the previous project. Recall that the ROC curve is created by plotting the false positive (FP) rate and the true positive (TP) rate over an interval of values of $\\theta$. Generally, a meaningful classifier has an area under the curve (AUC) value greater than 0.5, and an AUC value close to 1 signifies a near-perfect classifier. \n",
    "\n",
    "You may use the ``fp_tp_calc(file, theta)`` and ``auc_calc(file)`` functions from the previous project to calculate the AUC values. Additionally, we will let $\\theta \\in [0,10]$, and $ \\theta \\in \\mathbb{Z}$. \n",
    "\n",
    "<a id='question2'></a>\n",
    "# Question 2: Using previous functions\n",
    "\n",
    "Use the answer box below to paste in any functions that you would like to use for Question 2. You may also use this area to write any additional helper functions and/or edit your old functions to generalize to this problem and dataset (i.e., by adding a new parameter to handle the fact that r is an additional input variable). Labels for this dataset may be found in the file cardio.labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a5d998",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## PASTE ANY FUNCTIONS YOU WOULD LIKE TO USE HERE\n",
    "import subprocess\n",
    "from subprocess import run, PIPE\n",
    "\n",
    "##declare arguments list\n",
    "args = ['java', '-jar', '/usercode/negsel_src/negsel.jar','-n','21','-r','1', '-self', 'processed_cardio_train.csv']\n",
    "\n",
    "##open input test file to read from\n",
    "input_file = open('processed_cardio_test.csv', 'r')\n",
    "\n",
    "##run subprocess\n",
    "result = run(args, stdout=PIPE, stderr=PIPE, stdin=input_file, universal_newlines=True)\n",
    "\n",
    "##print results \n",
    "print(result.stdout)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348689e5-0e47-4fea-b865-82f0484bbf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2072.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def final_anom(file):\n",
    "    '''\n",
    "    Function to compute the final anomaly score for a given test set.\n",
    "    Inputs:\n",
    "        file: path to the test file\n",
    "    Output:\n",
    "        score: final anomaly score that is the log sum of all anomaly scores rounded to two decimals\n",
    "    '''\n",
    "\n",
    "    ##declare arguments list\n",
    "    args = ['java', '-jar', 'negsel.jar','-n','21','-r','4', '-self', 'processed_cardio_train.csv']\n",
    "    correction_value = 1e-6 \n",
    "    ##open input test file to read from\n",
    "    input_file = open(file, 'r')\n",
    "    \n",
    "    ##run subprocess\n",
    "    result = run(args, stdout=PIPE, stderr=PIPE, stdin=input_file, universal_newlines=True)\n",
    "    lines = result.stdout.splitlines()\n",
    "    ##print results \n",
    "    score = 0\n",
    "    for line in lines:\n",
    "        ##print(line)\n",
    "        alpha = float(line)\n",
    "        if alpha == 0:\n",
    "            alpha = correction_value\n",
    "        score = score + math.log(alpha)\n",
    "    \n",
    "    return score\n",
    "round(final_anom('processed_cardio_test.csv'),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d2530cf-1566-4f1e-a495-f996fe51ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_tp_calc(file, theta,r):\n",
    "    '''\n",
    "    Function to calculate the false positive and true positive rate for a given test set and value of theta.\n",
    "    Inputs: \n",
    "        file: path to the test file\n",
    "        theta: threshold value for ROC analysis\n",
    "    Output:\n",
    "        fp_tp : python tuple of the FPR and TPR given by (fp, tp)\n",
    "    '''\n",
    "     ##declare arguments list\n",
    "    args = ['java', '-jar', 'negsel.jar','-n','21','-r',str(r), '-self', 'sol_processed_cardio_train.csv']\n",
    "    correction_value = 1e-6 \n",
    "    ##open input test file to read from\n",
    "    input_file = open(file, 'r')\n",
    "    \n",
    "    ##run subprocess\n",
    "    result = run(args, stdout=PIPE, stderr=PIPE, stdin=input_file, universal_newlines=True)\n",
    "    lines = result.stdout.splitlines()\n",
    "    \n",
    "    ##print results \n",
    "    score = 0\n",
    "    count=0\n",
    "    tfp=0\n",
    "    ttp =0\n",
    "    for line in lines:\n",
    "        count+=1\n",
    "        ##print(line)\n",
    "        alpha = float(line)\n",
    "        \n",
    "        \n",
    "        if alpha > theta:\n",
    "            if count < 656:\n",
    "                tfp+=1\n",
    "            else:\n",
    "                ttp+=1\n",
    "                \n",
    "    \n",
    "    fp = tfp/655\n",
    "    tp = ttp/176\n",
    "    fp_tp = (fp,tp)\n",
    "    \n",
    "    return fp_tp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e3bede8-0b57-42da-b21a-802859b3123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def auc_calc(file,r):\n",
    "    '''\n",
    "    Function to calculate the AUC for a given test set over the theta interval of [0,10].\n",
    "    Inputs: \n",
    "        file: path to the test file\n",
    "    Output:\n",
    "        auc : area under the curve for the ROC curve of the test set\n",
    "    '''\n",
    "    fpr_array = list()\n",
    "    tpr_array = list()\n",
    "    for theta in range(11):\n",
    "        #print(theta)\n",
    "        fp_tp = fp_tp_calc(file,theta,r)\n",
    "        fpr_array.append(fp_tp[0])\n",
    "        tpr_array.append(fp_tp[1])\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    auc = metrics.auc(fpr_array, tpr_array)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14f923",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a id='question3'></a>\n",
    "# Question 3\n",
    "\n",
    "Write the function ``r_auc_tuple(file)`` that calculates a list of tuples for each value of $r\\in [2,10]$ over $\\theta \\in [0,10]$ for a given test file. This will result in a list of 10 tuples where the first element in each tuple is the value of $r$ and the second element is the AUC value.  This list will be returned as ``tuples_list`` and will be graded based on accuracy. The labels for the test file are in ``cardio.labels``.\n",
    "\n",
    "For reference, the following command should return the AUC value for $r=2$: ``r_auc_tuple('cardio_test.csv')[0][1]``.\n",
    "\n",
    "**Remember to use the ``process_data(file)`` function to run the Textor algorithm with the correct files.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b49d51f-ff72-4ed4-8313-e4d04eaccd73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from subprocess import run, PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71f9972e-c43b-4c7b-8f2d-0221046bd7fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def r_auc_tuple(file):\n",
    "    '''\n",
    "    Function to compute the auc values for various values of r.\n",
    "    Inputs: \n",
    "        file: path to the test file\n",
    "    Outputs:\n",
    "        tuples_list: a list of tuples where the first value in each tuple is r and the second value is the auc\n",
    "    '''\n",
    "    \n",
    "    # your code here\n",
    "    test_file = 'processed_cardio_test.csv'\n",
    "    tuples_list = []\n",
    "    for r in range(2,11):\n",
    "        auc = auc_calc(test_file,r)\n",
    "        \n",
    "        tupple = (r,auc)\n",
    "        tuples_list.append(tupple)\n",
    "    return tuples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "291b3465",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.7548143650242887), (3, 0.7168459403192228), (4, 0.2917071478140181), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0)]\n",
      "All test cases passed\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "You must run this cell.\n",
    "Check the output of your functions.\n",
    "'''\n",
    "import csv\n",
    "\n",
    "tuples = r_auc_tuple('cardio_test.csv')\n",
    "print(tuples)\n",
    "\n",
    "# don't edit the below code\n",
    "with open('/usercode/tuples.csv','w',newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(tuples)\n",
    "\n",
    "assert len(tuples) == 9\n",
    "assert round(tuples[0][1],2) == 0.75\n",
    "assert round(tuples[7][1],2) == 0.0\n",
    "print(\"All test cases passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92bb6f5-f67c-4c51-8827-48bff56a0f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - python3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
